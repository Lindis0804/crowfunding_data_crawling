{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in e:\\software\\anaconda\\lib\\site-packages (0.97.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in e:\\software\\anaconda\\lib\\site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4 in e:\\software\\anaconda\\lib\\site-packages (from fastapi) (1.10.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in e:\\software\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4->fastapi) (4.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in e:\\software\\anaconda\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\software\\anaconda\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\software\\anaconda\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (3.3)\n",
      "Requirement already satisfied: selenium in e:\\software\\anaconda\\lib\\site-packages (from -r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: webdriver_manager in e:\\software\\anaconda\\lib\\site-packages (from -r requirements.txt (line 2)) (3.8.6)\n",
      "Requirement already satisfied: pymongo in e:\\software\\anaconda\\lib\\site-packages (from -r requirements.txt (line 3)) (4.3.3)\n",
      "Requirement already satisfied: requests in e:\\software\\anaconda\\lib\\site-packages (from -r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: urllib3 in e:\\software\\anaconda\\lib\\site-packages (from -r requirements.txt (line 5)) (1.26.11)\n",
      "Requirement already satisfied: fastapi in e:\\software\\anaconda\\lib\\site-packages (from -r requirements.txt (line 6)) (0.97.0)\n",
      "Collecting kafka\n",
      "  Using cached kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
      "Collecting confluent_kafka\n",
      "  Using cached confluent_kafka-2.1.1-cp39-cp39-win_amd64.whl (3.4 MB)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in e:\\software\\anaconda\\lib\\site-packages (from selenium->-r requirements.txt (line 1)) (0.10.3)\n",
      "Requirement already satisfied: trio~=0.17 in e:\\software\\anaconda\\lib\\site-packages (from selenium->-r requirements.txt (line 1)) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in e:\\software\\anaconda\\lib\\site-packages (from selenium->-r requirements.txt (line 1)) (2022.9.14)\n",
      "Requirement already satisfied: packaging in e:\\software\\anaconda\\lib\\site-packages (from webdriver_manager->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: tqdm in e:\\software\\anaconda\\lib\\site-packages (from webdriver_manager->-r requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: python-dotenv in e:\\software\\anaconda\\lib\\site-packages (from webdriver_manager->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in e:\\software\\anaconda\\lib\\site-packages (from pymongo->-r requirements.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\software\\anaconda\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\software\\anaconda\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in e:\\software\\anaconda\\lib\\site-packages (from fastapi->-r requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4 in e:\\software\\anaconda\\lib\\site-packages (from fastapi->-r requirements.txt (line 6)) (1.10.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in e:\\software\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4->fastapi->-r requirements.txt (line 6)) (4.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in e:\\software\\anaconda\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: outcome in e:\\software\\anaconda\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in e:\\software\\anaconda\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in e:\\software\\anaconda\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in e:\\software\\anaconda\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.14 in e:\\software\\anaconda\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: sniffio in e:\\software\\anaconda\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in e:\\software\\anaconda\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (21.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in e:\\software\\anaconda\\lib\\site-packages (from trio-websocket~=0.9->selenium->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in e:\\software\\anaconda\\lib\\site-packages (from urllib3->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\software\\anaconda\\lib\\site-packages (from packaging->webdriver_manager->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: colorama in e:\\software\\anaconda\\lib\\site-packages (from tqdm->webdriver_manager->-r requirements.txt (line 2)) (0.4.5)\n",
      "Requirement already satisfied: pycparser in e:\\software\\anaconda\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->-r requirements.txt (line 1)) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in e:\\software\\anaconda\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r requirements.txt (line 1)) (0.14.0)\n",
      "Installing collected packages: kafka, confluent_kafka\n",
      "Successfully installed confluent_kafka-2.1.1 kafka-1.3.5\n"
     ]
    }
   ],
   "source": [
    "#uncomment this line if you want to install package\n",
    "!pip install -r requirements.txt\n",
    "#!pip install fastapi\n",
    "#pip install --upgrade pip\n",
    "#print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.common.exceptions\n",
    "import urllib3.exceptions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from time import sleep\n",
    "import threading\n",
    "import csv\n",
    "import traceback\n",
    "import multiprocessing\n",
    "from typing import Union\n",
    "from fastapi import FastAPI\n",
    "from conf_kafka.producer import ProjectProducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi', 2, 3], [4, 5]]\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "def split_list(lst, k):\n",
    "    \"\"\"\n",
    "    This function is used to split a list to sub-lists that have k items.\n",
    "\n",
    "    Args:\n",
    "        lst (list): input list\n",
    "        k (int): number of item of a sub-list\n",
    "\n",
    "    Returns:\n",
    "        list: list of sub-list\n",
    "    For example:\n",
    "    split_list([1,2,3,4,5],2) => [[1,2],[3,4],[5]]\n",
    "    \"\"\"\n",
    "    size = len(lst)\n",
    "    return [lst[i:i+k] for i in range(0, size, k)]\n",
    "\n",
    "\n",
    "print(split_list([\"hi\", 2, 3, 4, 5], 3))\n",
    "print([1,2,3,4,5,6][:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def get_first_not_null_item(lsts):\n",
    "    \"\"\"\n",
    "      this function is used to get the first not null item of a list, it there is no not-null item,\n",
    "      this func returns the last item\n",
    "      \n",
    "    Args:\n",
    "        lsts (list): input list\n",
    "\n",
    "    Returns:\n",
    "        string: the first not null item\n",
    "    \"\"\"\n",
    "    for l in lsts:\n",
    "        if (len(l)!=0):\n",
    "            return l\n",
    "    return lsts[-1]\n",
    "print(get_first_not_null_item(['','1','']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl url: \n",
      "https://www.kickstarter.com/projects/mlspencer/dragon-mage-deluxe-collectors-edition-hardcover\n",
      "[*] Sending to broker localhost:9092, topic: kickstarter_project\n",
      "{'title': \"Dragon Mage: Deluxe Collector's Edition Hardcover\", 'description': \"A deluxe, signed, faux-leather Collector's Edition of the first book in the multi-award-winning epic fantasy series Rivenworld.\", 'picture': 'https://ksr-ugc.imgix.net/assets/041/284/528/eaed1a1d708b34bbc525d42fe063f646_original.png?ixlib=rb-4.0.2&crop=faces&w=1024&h=576&fit=crop&v=1686684665&auto=format&frame=1&q=92&s=2a9491a0399da60e0010d30f6dde643d', 'pledged': 'US$ 60,627', 'goal': 'US$ 14,000', 'num_of_backer': '508', 'days_to_go': '14', 'mark': 'Project We Love', 'field': 'Fiction', 'location': 'Redlands, CA', 'num_of_comment': '55'}\n",
      "Send message synchronously\n",
      "[*] Send to kafka fail.\n"
     ]
    }
   ],
   "source": [
    "def get_detail_project(page, url,error_url_file,producer_info=[]):\n",
    "    \"\"\"\n",
    "    This func is used to crawl data of a detail project page of Kickstarter website\n",
    "\n",
    "    Args:\n",
    "        page (int): page index of url\n",
    "        url (string): link to website that need to be crawled\n",
    "        error_url_file (string): name of file that contains a list of error urls that can not be crawled (\n",
    "            this list will be executed later\n",
    "        )\n",
    "    \"\"\"\n",
    "    browser = webdriver.Chrome(\n",
    "        service=(Service(ChromeDriverManager().install()))\n",
    "    )\n",
    "    try:\n",
    "        print(\"crawl url: \")\n",
    "        print(url)\n",
    "        browser.get(url)\n",
    "        # sleep(2)\n",
    "        # t = title.text\n",
    "        wait = WebDriverWait(browser, 10)\n",
    "        title = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"h2.type-24-md.soft-black.mb1.project-name\"))))\n",
    "        description = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"p[class='type-14 type-18-md soft-black project-description mb1']\"))))\n",
    "        picture = browser.find_element(\n",
    "            By.CSS_SELECTOR, \"img[class='aspect-ratio--object bg-black z3']\").get_attribute(\"src\")\n",
    "        pledged = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"span[class='ksr-green-500']\"))))\n",
    "        span_goal = browser.find_elements(By.CSS_SELECTOR, \"span[class='inline-block-sm hide']\")\n",
    "        n_goal = []\n",
    "        for i in span_goal:\n",
    "            goals = i.find_elements(By.CSS_SELECTOR, \"span[class='money']\")\n",
    "            for goal in goals:\n",
    "                n_goal.append(goal.text)\n",
    "        goal = get_first_not_null_item(n_goal)\n",
    "        num_of_backer = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"div[class='block type-16 type-28-md bold dark-grey-500']\"))))\n",
    "        days_to_go = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"span[class='block type-16 type-28-md bold dark-grey-500']\"))))\n",
    "        mark_field_locations = list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR,\"a.nowrap.navy-700.flex.items-center.medium.mr3.type-12.keyboard-focusable > span.ml1\"\n",
    "        )))\n",
    "        mark = mark_field_locations[3]\n",
    "        field = mark_field_locations[4]\n",
    "        location = mark_field_locations[5]\n",
    "        num_of_comment = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"a.js-analytics-section.js-load-project-comments.js-load-project-content.mx3.project-nav__link--comments.tabbed-nav__link.type-14 > span.count\"\n",
    "        ))))\n",
    "        res = {\n",
    "            \"title\":title,\n",
    "            \"description\":description,\n",
    "            \"picture\":picture,\n",
    "            \"pledged\":pledged,\n",
    "            \"goal\":goal,\n",
    "            \"num_of_backer\":num_of_backer,\n",
    "            \"days_to_go\":days_to_go,\n",
    "            \"mark\":mark,\n",
    "            \"field\":field,\n",
    "            \"location\":location,\n",
    "            \"num_of_comment\":num_of_comment\n",
    "        }\n",
    "        if (len(producer_info)>0):\n",
    "            broker,topic = producer_info\n",
    "            print(\"[*] Sending to broker \"+broker+\", topic: \"+topic)\n",
    "            print(str(res))\n",
    "            projectProducer = ProjectProducer(broker=broker)\n",
    "            try:\n",
    "                projectProducer.send_msg_sync(res, topic=topic)\n",
    "                print(\"[*] Send to kafka successfully.\")\n",
    "            except:\n",
    "                print(\"[*] Send to kafka fail.\") \n",
    "        else:\n",
    "        #  file = open(\"./data/result.txt\",\"a\")\n",
    "        #  file.write(url+\"\\n\")\n",
    "        #  file.write(str(res)+\"\\n\")\n",
    "        #  file.close()\n",
    "          print(\"[*] Save data to mongodb.\")\n",
    "    except:\n",
    "        error_url = str(page)+\",\"+url+\"\\n\"\n",
    "        error_url_file_obj = open(error_url_file, \"a\")\n",
    "        error_url_file_obj.write(error_url)\n",
    "        error_url_file_obj.close()\n",
    "        traceback.print_exc()\n",
    "    browser.close()\n",
    "# get_data(current_page=current_page)\n",
    "get_detail_project(0,\"https://www.kickstarter.com/projects/mlspencer/dragon-mage-deluxe-collectors-edition-hardcover\",error_url_file=\"./data/error_url.csv\",producer_info=[\"localhost:9092\",\"kickstarter_project\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url,current_page,num_of_thread,error_url_file,checkpoint_file):\n",
    "    \"\"\"\n",
    "    This func is used to crawl data from Kickstarter website (\n",
    "        https://www.kickstarter.com/discover/advanced?woe_id=0&sort=magic&seed=2811224&page=\n",
    "    )\n",
    "\n",
    "    Args:\n",
    "        url (string): link to website\n",
    "        current_page (int): current page index of page that is being crawled\n",
    "        num_of_thread (int): num of crawling thread \n",
    "        error_url_file (string): name of file that contains a list of error_url_file that can not be crawled\n",
    "        checkpoint_file (string): name of file that contains information about the index of page that is being crawled\n",
    "    \"\"\"\n",
    "    page = current_page\n",
    "    while (1):\n",
    "        browser = webdriver.Chrome(\n",
    "        service=(Service(ChromeDriverManager().install()))\n",
    "    )\n",
    "        meta_url = url+str(page)\n",
    "        print(\"meta_url: \")\n",
    "        print(meta_url)\n",
    "        try:\n",
    "            browser.get(meta_url)\n",
    "            sleep(2)\n",
    "            links = list(map(lambda a: a.get_attribute(\"href\"),\n",
    "                                 browser.find_elements(By.CSS_SELECTOR,\n",
    "                                                       \"a[class='block img-placeholder w100p']\")\n",
    "                                 ))\n",
    "            prj_links = [l for l in links if l.endswith(\"?ref=discovery\")]\n",
    "            print(\"prj_links: \")\n",
    "            for l in prj_links:\n",
    "                print(l)\n",
    "            print(len(prj_links))\n",
    "            threads = []\n",
    "            split_prj_links = split_list(prj_links, num_of_thread)\n",
    "            last_prj_links = split_prj_links[-1]\n",
    "            print(\"split_prj_links: \")\n",
    "            print(split_prj_links)\n",
    "            for links in split_prj_links[:-1]:\n",
    "                threads = [threading.Thread(\n",
    "                        target=get_detail_project, args=(current_page, link,error_url_file)) for link in links]\n",
    "                for thread in threads:\n",
    "                    thread.start()\n",
    "                for thread in threads:\n",
    "                    thread.join()\n",
    "                threads = []\n",
    "            threads = [threading.Thread(\n",
    "                    target=get_detail_project, args=(current_page, link,error_url_file)) for link in last_prj_links]\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "            threads = []\n",
    "            page = page+1\n",
    "        except:\n",
    "            file = open(checkpoint_file, \"w\")\n",
    "            file.write(str({\"page\": page}))\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "        browser.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# link to the website Kickstarter\n",
    "url = 'https://www.kickstarter.com/discover/advanced?woe_id=0&sort=magic&seed=2811224&page='\n",
    "\n",
    "# get the current page\n",
    "checkpoint = eval(open('./data/checkpoint.csv', \"r\").readline())\n",
    "current_page = checkpoint[\"page\"]\n",
    "error_url_file = 'error_url.csv'\n",
    "print(current_page)\n",
    "\n",
    "# start to crawl\n",
    "app = FastAPI()\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option('detach',True)\n",
    "@app.get(\"/projects\")\n",
    "def get_data_from_kickstarter(num_of_thread:int = 4):\n",
    " get_data(current_page=current_page,url=url,num_of_thread=num_of_thread,checkpoint_file=\"./data/checkpoint.csv\",error_url_file=\"./data/error_url.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test some function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geats\n"
     ]
    }
   ],
   "source": [
    "print(\"geats\")\n",
    "browser = webdriver.Chrome(\n",
    "        service=(Service(ChromeDriverManager().install()))\n",
    "    )\n",
    "meta_url = url+str(4)\n",
    "browser.get(meta_url)\n",
    "sleep(1)\n",
    "browser.close()\n",
    "browser = webdriver.Chrome(\n",
    "        service=(Service(ChromeDriverManager().install()))\n",
    "    )\n",
    "browser.get(url+str(5))\n",
    "sleep(1)\n",
    "# get_data(current_page=current_page)\n",
    "# get_detail_project(0,\"https://www.kickstarter.com/projects/mlspencer/dragon-mage-deluxe-collectors-edition-hardcover\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "0\n",
      "[100, 2, 4]\n",
      "multi thread\n",
      "10000\n",
      "multi thread\n",
      "4\n",
      "multi thread\n",
      "9\n",
      "multi thread\n",
      "16\n",
      "multi thread\n",
      "25\n",
      "multi thread\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# f = open(file,\"a\")\n",
    "# f.writelines(\"hello\")\n",
    "# f.writelines(\"hi\")\n",
    "# f.close()\n",
    "# f = open(file,\"r\")\n",
    "# l = list(map(lambda a:a.strip().split(\",\"),f.readlines()))\n",
    "# print(l)\n",
    "print(\"hello\")\n",
    "x = ''\n",
    "print(len(x))\n",
    "def task(x):\n",
    "    print(\"multi thread\")\n",
    "    print(x * x)\n",
    "data = [[1,2],[2,3],[4,5]]\n",
    "inputs = [100,2,3,4,5,7]\n",
    "\n",
    "res = [i for i in inputs if i % 2==0]\n",
    "print(res)\n",
    "threads =[]\n",
    "# for m in inputs:\n",
    "#     thread = threading.Thread(target=task,args=[m])\n",
    "#     threads.append(thread)\n",
    "threads = [threading.Thread(target=task,args=[m]) for m in inputs]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "# pool.close()\n",
    "#pool.join()\n",
    "#print(\"results: {}\".format(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")\n",
    "for i in range(0, 5):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 2}\n",
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "x = None\n",
    "y = {\n",
    "    \"x\": x if x is not None else 2\n",
    "}\n",
    "print(y)\n",
    "m = [1,2,3]\n",
    "a,b,c = m\n",
    "print(a,b,c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
