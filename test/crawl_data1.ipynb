{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce5f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (4.10.0)\n",
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-3.8.6-py2.py3-none-any.whl (27 kB)\n",
      "Collecting pymongo\n",
      "  Downloading pymongo-4.4.0-cp310-cp310-win_amd64.whl (453 kB)\n",
      "     -------------------------------------- 453.6/453.6 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (1.26.14)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 57.0/57.0 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting kafka\n",
      "  Downloading kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
      "     -------------------------------------- 207.2/207.2 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting confluent_kafka\n",
      "  Downloading confluent_kafka-2.1.1-cp310-cp310-win_amd64.whl (3.4 MB)\n",
      "     ---------------------------------------- 3.4/3.4 MB 4.2 MB/s eta 0:00:00\n",
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "     -------------------------------------- 246.5/246.5 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 1)) (0.10.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 1)) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver_manager->-r requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver_manager->-r requirements.txt (line 2)) (22.0)\n",
      "Collecting dnspython<3.0.0,>=1.16.0\n",
      "  Using cached dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (2.0.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.9-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 4.8 MB/s eta 0:00:00\n",
      "Collecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 67.0/67.0 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4->fastapi->-r requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (22.1.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm->webdriver_manager->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->-r requirements.txt (line 1)) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r requirements.txt (line 1)) (0.14.0)\n",
      "Installing collected packages: kafka-python, kafka, confluent_kafka, python-dotenv, pydantic, dnspython, webdriver_manager, starlette, pymongo, fastapi\n",
      "Successfully installed confluent_kafka-2.1.1 dnspython-2.3.0 fastapi-0.98.0 kafka-1.3.5 kafka-python-2.0.2 pydantic-1.10.9 pymongo-4.4.0 python-dotenv-1.0.0 starlette-0.27.0 webdriver_manager-3.8.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "#!pip install fastapi\n",
    "#pip install --upgrade pip\n",
    "#print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c298b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.common.exceptions\n",
    "import urllib3.exceptions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from time import sleep\n",
    "import threading\n",
    "import csv\n",
    "import traceback\n",
    "import multiprocessing\n",
    "from typing import Union\n",
    "from fastapi import FastAPI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08189a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi', 2, 3], [4, 5]]\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "def split_list(lst, k):\n",
    "    \"\"\"\n",
    "    This function is used to split a list to sub-lists that have k items.\n",
    "\n",
    "    Args:\n",
    "        lst (list): input list\n",
    "        k (int): number of item of a sub-list\n",
    "\n",
    "    Returns:\n",
    "        list: list of sub-list\n",
    "    For example:\n",
    "    split_list([1,2,3,4,5],2) => [[1,2],[3,4],[5]]\n",
    "    \"\"\"\n",
    "    size = len(lst)\n",
    "    return [lst[i:i+k] for i in range(0, size, k)]\n",
    "\n",
    "\n",
    "print(split_list([\"hi\", 2, 3, 4, 5], 3))\n",
    "print([1,2,3,4,5,6][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6c85a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def get_first_not_null_item(lsts):\n",
    "    \"\"\"\n",
    "      this function is used to get the first not null item of a list, it there is no not-null item,\n",
    "      this func returns the last item\n",
    "      \n",
    "    Args:\n",
    "        lsts (list): input list\n",
    "\n",
    "    Returns:\n",
    "        string: the first not null item\n",
    "    \"\"\"\n",
    "    for l in lsts:\n",
    "        if (len(l)!=0):\n",
    "            return l\n",
    "    return lsts[-1]\n",
    "print(get_first_not_null_item(['','1','']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49337961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def get_second_not_null_item(lsts):\n",
    "    filtered_data = [x for x in lsts if x != '']\n",
    "    \n",
    "    if len(filtered_data) >= 2:\n",
    "        return filtered_data[1]\n",
    "    else:\n",
    "        return None\n",
    "print(get_second_not_null_item(['','1', '7','']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a073ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error_url_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m     browser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# get_data(current_page=current_page)\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m get_detail_project(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kickstarter.com/projects/mlspencer/dragon-mage-deluxe-collectors-edition-hardcover\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43merror_url_file\u001b[49m,producer_info\u001b[38;5;241m=\u001b[39m[])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'error_url_file' is not defined"
     ]
    }
   ],
   "source": [
    "def get_detail_project(page, url,error_url_file,producer_info=[]):\n",
    "    browser = webdriver.Chrome(\n",
    "        service=(Service(ChromeDriverManager().install()))\n",
    "    )\n",
    "    try:\n",
    "        print(\"crawl url: \")\n",
    "        print(url)\n",
    "        browser.get(url)\n",
    "        # sleep(2)\n",
    "        # t = title.text\n",
    "        wait = WebDriverWait(browser, 10)\n",
    "        title = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"basicsSection-title desktop t-h5--sansSerif\"))))\n",
    "        description = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"basicsSection-tagline desktop t-body--sansSerif-lg\"))))\n",
    "        pledged = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"basicsGoalProgress-amountSold t-h5--sansSerif t-weight--bold\"))))\n",
    "        span_goal = browser.find_elements(By.CSS_SELECTOR, \"basicsGoalProgress-progressDetails-detailsGoal-goalPercentageOrInitiallyRaised\")\n",
    "        percentage, value = span_goal.split(\" of \")\n",
    "        goal = value.strip()\n",
    "        num_of_backer = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"t-weight--medium\"))))\n",
    "        days_to_go = get_second_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"t-weight--medium\"))))\n",
    "        field = list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR,\"linkedLabelsList-tag-text\"\n",
    "        )))\n",
    "        location = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR, \"basicsCampaignOwner-details-city\"))))\n",
    "        \n",
    "        num_of_comment = get_first_not_null_item(list(map(lambda a:a.text,browser.find_elements(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"tabHeadersWithPill-tab-pill t-label--sm\"\n",
    "        ))))\n",
    "        res = {\n",
    "            \"title\":title,\n",
    "            \"description\":description,\n",
    "            \"pledged\":pledged,\n",
    "            \"goal\":goal,\n",
    "            \"num_of_backer\":num_of_backer,\n",
    "            \"days_to_go\":days_to_go,\n",
    "            \"field\":field,\n",
    "            \"location\":location,\n",
    "            \"num_of_comment\":num_of_comment\n",
    "        }\n",
    "        if (len(producer_info)>0):\n",
    "            broker,topic = producer_info\n",
    "            print(\"[*] Sending to broker \"+broker+\", topic: \"+topic)\n",
    "            print(json.dumps(res).encode(\"utf-8\"))\n",
    "            projectProducer = ProjectProducer(broker=broker)\n",
    "            try:\n",
    "                projectProducer.send_msg_sync(json.dumps(res).encode(\"utf-8\"), topic=topic)\n",
    "                print(\"[*] Send to kafka successfully.\")\n",
    "            except:\n",
    "                print(\"[*] Send to kafka fail.\") \n",
    "        else:\n",
    "        #  file = open(\"./data/result.txt\",\"a\")\n",
    "        #  file.write(url+\"\\n\")\n",
    "        #  file.write(str(res)+\"\\n\")\n",
    "        #  file.close()\n",
    "          print(\"[*] Save data to mongodb.\")\n",
    "    except:\n",
    "        error_url = str(page)+\",\"+url+\"\\n\"\n",
    "        error_url_file_obj = open(error_url_file, \"a\")\n",
    "        error_url_file_obj.write(error_url)\n",
    "        error_url_file_obj.close()\n",
    "        traceback.print_exc()\n",
    "    browser.close()\n",
    "# get_data(current_page=current_page)\n",
    "get_detail_project(0,\"https://www.kickstarter.com/projects/mlspencer/dragon-mage-deluxe-collectors-edition-hardcover\",error_url_file,producer_info=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86305c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker,topic = \"localhost:9092\",\"kickstarter_project\"\n",
    "print(\"[*] Sending to broker \"+broker+\", topic: \"+topic)\n",
    "msg = \"Oh my gosh.\"\n",
    "print(msg)\n",
    "projectProducer = ProjectProducer(broker=broker)\n",
    "try:\n",
    "    projectProducer.send_msg_sync(msg, topic=topic)\n",
    "    print(\"[*] Send to kafka successfully.\")\n",
    "except:\n",
    "    print(\"[*] Send to kafka fail.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabec6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url,current_page,num_of_thread,error_url_file,checkpoint_file):\n",
    "    \"\"\"\n",
    "    This func is used to crawl data from Kickstarter website (\n",
    "        https://www.kickstarter.com/discover/advanced?woe_id=0&sort=magic&seed=2811224&page=\n",
    "    )\n",
    "\n",
    "    Args:\n",
    "        url (string): link to website\n",
    "        current_page (int): current page index of page that is being crawled\n",
    "        num_of_thread (int): num of crawling thread \n",
    "        error_url_file (string): name of file that contains a list of error_url_file that can not be crawled\n",
    "        checkpoint_file (string): name of file that contains information about the index of page that is being crawled\n",
    "    \"\"\"\n",
    "    page = current_page\n",
    "    while (1):\n",
    "        browser = webdriver.Chrome(\n",
    "        service=(Service(ChromeDriverManager().install()))\n",
    "    )\n",
    "        meta_url = url+str(page)\n",
    "        print(\"meta_url: \")\n",
    "        print(meta_url)\n",
    "        try:\n",
    "            browser.get(meta_url)\n",
    "            sleep(2)\n",
    "            links = list(map(lambda a: a.get_attribute(\"href\"),\n",
    "                                 browser.find_elements(By.CSS_SELECTOR,\n",
    "                                                       \"a[class='block img-placeholder w100p']\")\n",
    "                                 ))\n",
    "            prj_links = [l for l in links if l.endswith(\"?ref=discovery\")]\n",
    "            print(\"prj_links: \")\n",
    "            for l in prj_links:\n",
    "                print(l)\n",
    "            print(len(prj_links))\n",
    "            threads = []\n",
    "            split_prj_links = split_list(prj_links, num_of_thread)\n",
    "            last_prj_links = split_prj_links[-1]\n",
    "            print(\"split_prj_links: \")\n",
    "            print(split_prj_links)\n",
    "            for links in split_prj_links[:-1]:\n",
    "                threads = [threading.Thread(\n",
    "                        target=get_detail_project, args=(current_page, link,error_url_file)) for link in links]\n",
    "                for thread in threads:\n",
    "                    thread.start()\n",
    "                for thread in threads:\n",
    "                    thread.join()\n",
    "                threads = []\n",
    "            threads = [threading.Thread(\n",
    "                    target=get_detail_project, args=(current_page, link,error_url_file)) for link in last_prj_links]\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "            threads = []\n",
    "            page = page+1\n",
    "        except:\n",
    "            file = open(checkpoint_file, \"w\")\n",
    "            file.write(str({\"page\": page}))\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "        browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link to the website Kickstarter\n",
    "url = 'https://www.indiegogo.com/explore/all?project_type=campaign&project_timing=all&sort=trending'\n",
    "\n",
    "# get the current page\n",
    "checkpoint = eval(open('./data/checkpoint.csv', \"r\").readline())\n",
    "current_page = checkpoint[\"page\"]\n",
    "error_url_file = 'error_url.csv'\n",
    "print(current_page)\n",
    "\n",
    "# start to crawl\n",
    "app = FastAPI()\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option('detach',True)\n",
    "@app.get(\"/projects\")\n",
    "def get_data_from_kickstarter(num_of_thread:int = 4):\n",
    " get_data(current_page=current_page,url=url,num_of_thread=num_of_thread,checkpoint_file=\"./data/checkpoint.csv\",error_url_file=\"./da"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
